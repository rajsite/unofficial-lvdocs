<!DOCTYPE html
  PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<html lang="en-us" xml:lang="en-us">
   <head>
      <meta http-equiv="Content-Type" content="text/html; charset=Windows-1252">
      <link rel="stylesheet" type="text/css" href="htmlhelp_base.css">
      <title>General LS Linear Fit Theory</title><script src="jQuery-1.7.2.min.js" type="text/javascript" xml:space="preserve"></script><script src="linking.js" type="text/javascript" xml:space="preserve"></script><script type="text/javascript" xml:space="preserve">
		var fbProductDNA=2543;
		var fbVersionName="2012";
		</script><script src="feedbacklink1.0.js" type="text/javascript" xml:space="preserve"></script></head>
   <body><a name="GUID-B27FD1A9-862E-4620-9AA9-6515DDD6929F" shape="rect">
         <!-- --></a> 
      
      <h1 class="topictitle1">General LS Linear Fit Theory</h1> 
      
      <div> 
         	 
         <p> 
            		 For a given set of observation data, the general least-squares (LS) linear fit problem is to find a set of coefficients
            that fits the linear model, as shown in the following equation: 
            	 
         </p> 
         <div class="fignone"><a name="GUID-B27FD1A9-862E-4620-9AA9-6515DDD6929F__NOLOC_EQ_GENERAL_LS_LINEAR_FIT_THEORY" shape="rect">
               <!-- --></a><div class="mathml-block"><span><img src="./eneral_LS_Linear_Fit_Theory.xml_d3651e20.png"></span></div>
         </div> 
         	 
         <p>where 
            	 
         </p> 
         	 
         <p><em>x</em><sub><em>i</em><em>j</em></sub> is the observed data contained in the observation matrix <em>H</em>, 
            	 
         </p> 
         	 
         <p><em>n</em> is the number of elements in the set of observed data and the number of rows of in <em>H</em>, 
            	 
         </p> 
         	 
         <p><em>b</em> is the set of coefficients that fit the linear model, 
            	 
         </p> 
         	 
         <p><em>k</em> is the number of coefficients. 
            	 
         </p> 
         	 
         <p>The following equation defines the observation matrix <em>H</em>: 
            	 
         </p> 
         <div class="fignone">
            <div class="mathml-block"><span><img src="./eneral_LS_Linear_Fit_Theory.xml_d3651e192.png"></span></div>
         </div> 
         	 
         <p>You can rewrite the general LS linear fit model as the following equation: 
            	 
         </p> 
         <div class="fignone">
            <div class="mathml-block"><span><img src="./eneral_LS_Linear_Fit_Theory.xml_d3651e337.png"></span></div>
         </div> 
         	 
         <p>The general LS linear fit model is a multiple linear regression model. A multiple linear regression model uses several variables,
            <em>x</em><sub><em>i</em>0</sub>, <em>x</em><sub><em>i</em>1</sub>, …, <em>x</em><sub><em>i</em><em>k</em> - 1</sub>, to predict one variable, <em>y</em><sub><em>i</em></sub>. 
            	 
         </p> 
         	 
         <p>In most analysis situations, you acquire more observation data than coefficients. The general LS linear fit model might not
            yield all the coefficients in set <em>B</em>. The fit problem becomes to find the coefficient set <em>B</em> that minimizes the difference between the observed data <em>y</em><sub><em>i</em></sub> and the predicted value <em>z</em><sub><em>i</em></sub>. The following equation defines <em>z</em><sub><em>i</em></sub>. 
            	 
         </p> 
         <div class="fignone">
            <div class="mathml-block"><span><img src="./eneral_LS_Linear_Fit_Theory.xml_d3651e412.png"></span></div>
         </div> 
         	 
         <p>You can use the least chi-square plane method to find the solution set <em>B</em> that minimizes the quantity given by the following equation. 
            	 
         </p> 
         <div class="fignone">
            <div class="mathml-block"><span><img src="./eneral_LS_Linear_Fit_Theory.xml_d3651e468.png"></span></div>
         </div> 
         	 
         <p>where <em>h</em><sub><em>o</em><em>i</em><em>j</em></sub> = (<em>x</em><sub><em>i</em><em>j</em></sub>/&#963;<sub><em>i</em></sub>), <em>y</em><sub><em>o</em><em>i</em></sub> = (<em>y</em><sub><em>i</em></sub>/&#963;<sub><em>i</em></sub>) 
            	 
         </p> 
         	 
         <p>for <em>i</em> = 0, 1, …, <em>n</em> - 1, and <em>j</em> = 0, 1, …, <em>k</em> - 1. 
            	 
         </p> 
         	 
         <p>In the previous equation, &#963;<sub><em>i</em></sub> is the standard deviation. If the measurement errors are independent and normally distributed with constant standard deviation,
            &#963;<sub><em>i</em></sub> = &#963;, the previous equation also is the least-square estimation. 
            	 
         </p> 
         	 
         <p>You can minimize &#967;<sup>2</sup> from the previous equation in the following ways: 
            	 
         </p> 
         	 <a name="GUID-B27FD1A9-862E-4620-9AA9-6515DDD6929F__UL_9BD5DDE55AC74D73B05E9D392CF41D54" shape="rect">
            <!-- --></a><ul id="GUID-B27FD1A9-862E-4620-9AA9-6515DDD6929F__UL_9BD5DDE55AC74D73B05E9D392CF41D54">
            <li><a name="GUID-B27FD1A9-862E-4620-9AA9-6515DDD6929F__LI_80D44A876997423084FCDF602263FAF9" shape="rect">
                  <!-- --></a>Solve normal equations of the least-square problems using LU or Cholesky factorization. 
               		
            </li>
            <li><a name="GUID-B27FD1A9-862E-4620-9AA9-6515DDD6929F__LI_D9FF4F73FB764E2AA2FFAC153A866911" shape="rect">
                  <!-- --></a>Minimize &#967;<sup>2</sup> to find the least-square solution of equations. 
               		
            </li>
         </ul> 
         	 
         <p>To solve normal equations, you first set the partial derivatives of &#967;<sup>2</sup> to zero with respect to <em>b</em><sub>0</sub>, <em>b</em><sub>1</sub>, …, <em>b</em><sub><em>k</em>-1</sub>, as shown by the following equations: 
            	 
         </p> 
         <div class="fignone">
            <div class="mathml-block"><span><img src="./eneral_LS_Linear_Fit_Theory.xml_d3651e763.png"></span></div>
         </div> 
         	 
         <p>You then derive the equations in the previous equation to the following equation form: 
            	 
         </p> 
         <div class="fignone">
            <div class="mathml-block"><span><img src="./eneral_LS_Linear_Fit_Theory.xml_d3651e875.png"></span></div>
         </div> 
         	 
         <p>where <em>H</em><sub>0</sub><sup>T</sup> is the transpose of <em>H</em> 
            		<sub>0</sub>. 
            	 
         </p> 
         	 
         <p>Equations of the form given by the previous equation are called normal equations of the least-square problems. You can solve
            them using LU or Cholesky factorization algorithms. However, the solution from the normal equations is susceptible to round-off
            error. 
            	 
         </p> 
         	 
         <p>The preferred method of minimizing &#967;<sup>2</sup> is to find the least-square solution of equations. The following equation defines the form of the least-square solution of
            equations. 
            	 
         </p> 
         <div class="fignone">
            <div class="mathml-block"><span><img src="./eneral_LS_Linear_Fit_Theory.xml_d3651e938.png"></span></div>
         </div> 
         	 
         <p>You can use QR or SVD factorization to find the solution set B for the previous equation. For QR factorization, you can use
            the Householder algorithm, the Givens algorithm, or the Givens 2 algorithm, which also is known as the fast Givens algorithm.
            Different algorithms can give you different precision. In some cases, if one algorithm cannot solve the equation, another
            algorithm might solve it. You can try different algorithms to find the one best suited for the observation data. 
            	 
         </p> 
         
      </div> 
      
      <div>
         <div class="familylinks">
            <div class="parentlink"><strong>Parent topic:</strong> <a href="Curve_Fitting_Overview.html" shape="rect">Curve Fitting</a></div>
         </div>
      </div>
      
   </body>
</html>